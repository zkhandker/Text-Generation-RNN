{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nikhil-Scratch1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i-stTnVO1CR4"
      ],
      "authorship_tag": "ABX9TyOKYTjxDp1Qaftdh1SDBPR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zkhandker/rupi-kaur/blob/main/Nikhil_Scratch1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-stTnVO1CR4"
      },
      "source": [
        "### Playing Around with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8coD_CyNFkt"
      },
      "source": [
        "#Install word2vec and import usual libraries\n",
        "\n",
        "!pip install -q tqdm\n",
        "\n",
        "import io\n",
        "import itertools\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "SEED = 42 #no idea what these 2 lines do, but they have it in the word2vec documentation\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reltVnhJSiO2"
      },
      "source": [
        "Let's work through the example in the word2vec documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltdXZ75IQAJT",
        "outputId": "fdb025ba-ba8f-4c66-c0fb-4987af353905"
      },
      "source": [
        "#let's look at an example sentence.\n",
        "\n",
        "sentence = \"The wide road shimmered in the hot sun\"\n",
        "tokens = list(sentence.lower().split())\n",
        "print(len(tokens))\n",
        "print(tokens)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "['the', 'wide', 'road', 'shimmered', 'in', 'the', 'hot', 'sun']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS9iAGsAShN9",
        "outputId": "8aa92228-153c-4fca-829f-d5e1a398ed90"
      },
      "source": [
        "#map each word to an index\n",
        "\n",
        "vocab, index = {}, 1 # start indexing from 1\n",
        "vocab['<pad>'] = 0 # add a padding token \n",
        "for token in tokens:\n",
        "  if token not in vocab: \n",
        "    vocab[token] = index\n",
        "    # print(token)\n",
        "    index += 1\n",
        "vocab_size = len(vocab)\n",
        "print(vocab)\n",
        "\n",
        "#and its inverse\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "print(inverse_vocab)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n",
            "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfELRjWdVtHt",
        "outputId": "f3643cc4-d4bd-4429-abd7-9246a1183d54"
      },
      "source": [
        "#we can map the sentence to a vector\n",
        "#note that \"the\" is repeated in the sentence\n",
        "example_sequence = [vocab[word] for word in tokens]\n",
        "print(example_sequence)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 1, 6, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0xPKGmCW5bp",
        "outputId": "b9865c45-9577-4c14-c20e-dcc021e2836e"
      },
      "source": [
        "#now we need to come up with \"skip grams\"\n",
        "window_size = 2\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      example_sequence, \n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=window_size,\n",
        "      negative_samples=0)\n",
        "print(len(positive_skip_grams))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX3K-l-QeXiG",
        "outputId": "8e27bf40-0480-46cd-febc-e68d436115e1"
      },
      "source": [
        "print(positive_skip_grams)\n",
        "for target, context in positive_skip_grams[:5]:\n",
        "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7, 1], [4, 1], [4, 3], [3, 4], [5, 1], [3, 5], [2, 4], [4, 5], [1, 7], [6, 5], [2, 3], [6, 1], [3, 2], [2, 1], [3, 1], [5, 3], [1, 6], [1, 2], [4, 2], [6, 7], [5, 4], [7, 6], [5, 6], [1, 5], [1, 4], [1, 3]]\n",
            "(7, 1): (sun, the)\n",
            "(4, 1): (shimmered, the)\n",
            "(4, 3): (shimmered, road)\n",
            "(3, 4): (road, shimmered)\n",
            "(5, 1): (in, the)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vk4fdFmeft4x",
        "outputId": "f3136083-9738-44bd-b7b6-d28caac1120b"
      },
      "source": [
        "# Get target and context words for one positive skip-gram.\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "# Set the number of negative samples per positive context. \n",
        "num_ns = 4\n",
        "\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class, # class that should be sampled as 'positive'\n",
        "    num_true=1, # each positive skip-gram has 1 positive context class\n",
        "    num_sampled=num_ns, # number of negative context words to sample\n",
        "    unique=True, # all the negative samples should be unique\n",
        "    range_max=vocab_size, # pick index of the samples from [0, vocab_size]\n",
        "    seed=SEED, # seed for reproducibility\n",
        "    name=\"negative_sampling\" # name of this operation\n",
        ")\n",
        "print(negative_sampling_candidates)\n",
        "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([2 1 4 3], shape=(4,), dtype=int64)\n",
            "['wide', 'the', 'shimmered', 'road']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmCyI67rhMuK"
      },
      "source": [
        "# Add a dimension so you can use concatenation (on the next step).\n",
        "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
        "\n",
        "# Concat positive context word with negative sampled words.\n",
        "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "\n",
        "# Label first context word as 1 (positive) followed by num_ns 0s (negative).\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\") \n",
        "\n",
        "# Reshape target to shape (1,) and context and label to (num_ns+1,).\n",
        "target = tf.squeeze(target_word)\n",
        "context = tf.squeeze(context)\n",
        "label =  tf.squeeze(label)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWeHlbkUg3yX",
        "outputId": "c9a81237-aae2-4e54-8c38-fbb0d876c666"
      },
      "source": [
        "print(f\"target_index    : {target}\")\n",
        "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
        "print(f\"context_indices : {context}\")\n",
        "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
        "print(f\"label           : {label}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target_index    : 7\n",
            "target_word     : sun\n",
            "context_indices : [1 2 1 4 3]\n",
            "context_words   : ['the', 'wide', 'the', 'shimmered', 'road']\n",
            "label           : [1 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WILxfANXiH8D",
        "outputId": "66be41ab-1a76-4413-9cfc-a10225447e4f"
      },
      "source": [
        "print(f\"target  :\", target)\n",
        "print(f\"context :\", context )\n",
        "print(f\"label   :\", label )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target  : tf.Tensor(7, shape=(), dtype=int32)\n",
            "context : tf.Tensor([1 2 1 4 3], shape=(5,), dtype=int64)\n",
            "label   : tf.Tensor([1 0 0 0 0], shape=(5,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ignP-2sRiH0q",
        "outputId": "a29640e4-eb92-4187-9bbd-3e725bbbab36"
      },
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
        "print(sampling_table)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n",
            " 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxf_RRTBioVm"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBQThUfoioTN"
      },
      "source": [
        "#get data\n",
        "path_to_file = tf.keras.utils.get_file('combined2', 'https://raw.githubusercontent.com/zkhandker/rupi-kaur/main/data/combined2.txt')"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zolHgIqxioRI",
        "outputId": "05580d48-0e2b-4399-b46c-f595226aadec"
      },
      "source": [
        "with open(path_to_file) as f: \n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how is it so easy for you\n",
            "to be kind to people he asked\n",
            "\n",
            "milk and honey dripped\n",
            "from my lips as i answered\n",
            "\n",
            "cause people have not\n",
            "been kind to me\n",
            "the first boy that kissed me\n",
            "held my shoulders down\n",
            "like the handlebars of\n",
            "the first bicycle\n",
            "he ever rode\n",
            "i was five\n",
            "\n",
            "he had the smell of\n",
            "starvation on his lips\n",
            "which he picked up from\n",
            "his father feasting on his mother at 4 a.m.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfnMprADqRw4",
        "outputId": "17c79bbd-3ae4-4e7c-9717-e2093b734a3e"
      },
      "source": [
        "print(path_to_file)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.keras/datasets/combined2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLPKfUqqioOF"
      },
      "source": [
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugvB_xnNioLk"
      },
      "source": [
        "# We create a custom standardization function to lowercase the text and \n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5yxxcU0ioI-"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))\n",
        "# import sys\n",
        "# sys.setdefaultencoding('ISO-8859-1')\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFBNgBNiioGJ",
        "outputId": "391cb25e-c1a1-4c1f-9e9a-dd8409a83185"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'you', 'the', 'i', 'to', 'and', 'of', 'is', 'it', 'me', 'your', 'my', 'a', 'in', 'not', 'that', 'when', 'with', 'for']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96oL0mfzx3L-",
        "outputId": "50e89948-3872-45b3-b095-844b6d311a6a"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))\n",
        "\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "\n",
        "\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsLbdqXNx3Gc",
        "outputId": "f82da088-3d13-493e-e47d-f56de76db907"
      },
      "source": [
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 21   8   9  25 180  19   2   0   0   0] => ['how', 'is', 'it', 'so', 'easy', 'for', 'you', '', '', '']\n",
            "[   5   23  353    5   80   31 1330    0    0    0] => ['to', 'be', 'kind', 'to', 'people', 'he', 'asked', '', '', '']\n",
            "[ 499    6  149 1192    0    0    0    0    0    0] => ['milk', 'and', 'honey', 'dripped', '', '', '', '', '', '']\n",
            "[  72   12   90   36    4 1340    0    0    0    0] => ['from', 'my', 'lips', 'as', 'i', 'answered', '', '', '', '']\n",
            "[67 80 27 15  0  0  0  0  0  0] => ['cause', 'people', 'have', 'not', '', '', '', '', '', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhrZnXI2x3Cp",
        "outputId": "9973c879-5f7b-40d1-d308-22dbac3a6f1a"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2568/2568 [00:00<00:00, 6812.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2896 2896 2896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DEIHPaBx2-u",
        "outputId": "d6203950-cf2b-4d68-fd7c-3b4ed96ac0bc"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTcO4jfix26W",
        "outputId": "966a5391-e391-4574-adb1-dc836d0928ff"
      },
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SraQNdb5x2jJ"
      },
      "source": [
        "\n",
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fx312mv_ycQN"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrua1slCycNq"
      },
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0jwq8VcycKE"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkzoqKw3ycGF",
        "outputId": "761dd260-eafb-425e-ad8b-f5e17efb3db3"
      },
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 1s 83ms/step - loss: 1.6095 - accuracy: 0.2051\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.6044 - accuracy: 0.3890\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.5996 - accuracy: 0.5570\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.5948 - accuracy: 0.7204\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.5900 - accuracy: 0.8219\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.5849 - accuracy: 0.8805\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1.5797 - accuracy: 0.9141\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.5743 - accuracy: 0.9339\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.5686 - accuracy: 0.9541\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1.5627 - accuracy: 0.9671\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.5563 - accuracy: 0.9704\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.5496 - accuracy: 0.9756\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.5425 - accuracy: 0.9818\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.5349 - accuracy: 0.9837\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.5268 - accuracy: 0.9860\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.5182 - accuracy: 0.9873\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1.5090 - accuracy: 0.9886\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.4992 - accuracy: 0.9886\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.4889 - accuracy: 0.9889\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.4779 - accuracy: 0.9889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff34570c750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGoyH4qWy3GF"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAvTMTzvy3DG"
      },
      "source": [
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if  index == 0: continue # skip 0, it's padding.\n",
        "  vec = weights[index] \n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pPFXxLgcy2_k",
        "outputId": "9cf6f287-e143-4ad2-ad84-3d762000c12f"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception as e:\n",
        "  pass"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_06c77736-2ea5-4eb9-bc5a-9df57ec64696\", \"vectors.tsv\", 2210782)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9323be4a-62e4-448e-9e20-d7f5ec1430f9\", \"metadata.tsv\", 9492)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBTNl7dg066D"
      },
      "source": [
        "## Now generate text - try doing it by characters first?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfNmJw3r1VWw",
        "outputId": "093ddaff-3e8f-4990-edb9-2970560ed332"
      },
      "source": [
        "# Download the dataset\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 66688 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqPeOqT-1VTu",
        "outputId": "8cf33e14-f532-4e8b-a8a7-05efda7b400c"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how is it so easy for you\n",
            "to be kind to people he asked\n",
            "\n",
            "milk and honey dripped\n",
            "from my lips as i answered\n",
            "\n",
            "cause people have not\n",
            "been kind to me\n",
            "the first boy that kissed me\n",
            "held my shoulders down\n",
            "like the handlebars of\n",
            "the first bicycle\n",
            "he ever rod\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-D0l64D1VR1",
        "outputId": "8eed7bdc-c3bf-4b7b-8535-b7540aae3f27"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Te610WN1VPD",
        "outputId": "90db44fe-ad76-4061-b6bf-58c0bfca0b09"
      },
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAE4Z0pV1VM7"
      },
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab))"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDzARrFP1VKC",
        "outputId": "bee28299-2ead-44e4-b255-8d3b6f8c4b12"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (32, None, 256)           12288     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (32, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (32, None, 48)            49200     \n",
            "=================================================================\n",
            "Total params: 5,308,464\n",
            "Trainable params: 5,308,464\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgGk7HSr3oKI",
        "outputId": "374ae42f-0e01-403d-dd1e-65214a748c37"
      },
      "source": [
        "print(vectorize_text(text))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 21   8   9  25 180  19   2   5  23 353], shape=(10,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CXMH_4q3nCq"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvsIk2xf1VHH",
        "outputId": "79217468-adc7-42a1-d583-3e01635ecd64"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJtDUTm91U-O"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZrWqUDLHAvY",
        "outputId": "cdba6ffc-fc1a-4f2c-df05-ffb0d68ac810"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(66688,), dtype=int64, numpy=array([29, 36, 44, ..., 26, 39,  2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPK61yirHAtV",
        "outputId": "8900c678-c860-4acc-e4b0-4234d5a8b613"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h\n",
            "o\n",
            "w\n",
            " \n",
            "i\n",
            "s\n",
            " \n",
            "i\n",
            "t\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSrCemW0HAq6",
        "outputId": "df235fd5-1d1b-4fc4-c174-e9545316959d"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'h' b'o' b'w' b' ' b'i' b's' b' ' b'i' b't' b' ' b's' b'o' b' ' b'e'\n",
            " b'a' b's' b'y' b' ' b'f' b'o' b'r' b' ' b'y' b'o' b'u' b'\\n' b't' b'o'\n",
            " b' ' b'b' b'e' b' ' b'k' b'i' b'n' b'd' b' ' b't' b'o' b' ' b'p' b'e'\n",
            " b'o' b'p' b'l' b'e' b' ' b'h' b'e' b' ' b'a' b's' b'k' b'e' b'd' b'\\n'\n",
            " b'\\n' b'm' b'i' b'l' b'k' b' ' b'a' b'n' b'd' b' ' b'h' b'o' b'n' b'e'\n",
            " b'y' b' ' b'd' b'r' b'i' b'p' b'p' b'e' b'd' b'\\n' b'f' b'r' b'o' b'm'\n",
            " b' ' b'm' b'y' b' ' b'l' b'i' b'p' b's' b' ' b'a' b's' b' ' b'i' b' '\n",
            " b'a' b'n' b's'], shape=(101,), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egmr837PHAnT",
        "outputId": "babd1c87-324d-41b4-8b06-0057ad252dd7"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'how is it so easy for you\\nto be kind to people he asked\\n\\nmilk and honey dripped\\nfrom my lips as i ans'\n",
            "b'wered\\n\\ncause people have not\\nbeen kind to me\\nthe first boy that kissed me\\nheld my shoulders down\\nlike'\n",
            "b' the handlebars of\\nthe first bicycle\\nhe ever rode\\ni was five\\n\\nhe had the smell of\\nstarvation on his l'\n",
            "b'ips\\nwhich he picked up from\\nhis father feasting on his mother at 4 a.m.\\n\\nhe was the first boy\\nto teac'\n",
            "b'h me my body was\\nfor giving to those that wanted\\nthat i should feel anything\\nless than whole\\n\\nand my '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqNiTi3oHAks"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tim-2DkWHAin",
        "outputId": "09571122-e1f9-4436-dc2c-5578fa510ea2"
      },
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : b'how is it so easy for you\\nto be kind to people he asked\\n\\nmilk and honey dripped\\nfrom my lips as i an'\n",
            "Target: b'ow is it so easy for you\\nto be kind to people he asked\\n\\nmilk and honey dripped\\nfrom my lips as i ans'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvwpnNouHAfE",
        "outputId": "97683a9f-f08c-49ae-f6ff-243cd3fd475e"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZGsfMEHAbp"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True, \n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else: \n",
        "      return x\n",
        "\n",
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw9A0DLiHvHD",
        "outputId": "e9be4f0b-e33e-4282-f5f1-762f9f8136ae"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 50) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrB_hAFCHvES",
        "outputId": "e333e95c-d70e-4b6b-974c-7608bd5d63a1"
      },
      "source": [
        "model.summary()\n"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      multiple                  12800     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  51250     \n",
            "=================================================================\n",
            "Total params: 4,002,354\n",
            "Trainable params: 4,002,354\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkLjf_DjHvBA",
        "outputId": "0e5b4bd6-7f32-4a49-e473-766714bf3547"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'heir\\nbody and leave after saying you will find better than me.\\n\\nyou will stand there naked with half'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"q:'xdv47nw,\\xc3\\xadheuo1\\n5zcqwwmkma9i4rx0*[UNK]'bk6-40vexaa3u:ytt5j8. ck0hp(*t40jr6t0g\\ni9-oc)na(c0rxn9q80wo*.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2RZ3yzUH_q3",
        "outputId": "ac6cae40-4fb0-448e-d8de-df32ed311a3a"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 50)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         3.914017\n",
            "Epoch 1/20\n",
            "10/10 [==============================] - 53s 5s/step - loss: 4.0866\n",
            "Epoch 2/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 3.6542\n",
            "Epoch 3/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 3.1590\n",
            "Epoch 4/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.9035\n",
            "Epoch 5/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.7440\n",
            "Epoch 6/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.5793\n",
            "Epoch 7/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.4586\n",
            "Epoch 8/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.3793\n",
            "Epoch 9/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.3350\n",
            "Epoch 10/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.2928\n",
            "Epoch 11/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.2564\n",
            "Epoch 12/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.2260\n",
            "Epoch 13/20\n",
            "10/10 [==============================] - 50s 5s/step - loss: 2.1836\n",
            "Epoch 14/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.1582\n",
            "Epoch 15/20\n",
            "10/10 [==============================] - 50s 5s/step - loss: 2.1131\n",
            "Epoch 16/20\n",
            "10/10 [==============================] - 50s 5s/step - loss: 2.0795\n",
            "Epoch 17/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 2.0463\n",
            "Epoch 18/20\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.0051\n",
            "Epoch 19/20\n",
            "10/10 [==============================] - 51s 5s/step - loss: 1.9709\n",
            "Epoch 20/20\n",
            "10/10 [==============================] - 50s 5s/step - loss: 1.9443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loF2zGZIH_oL"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQp88tLJMXWY"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VsYiG-HH_lw",
        "outputId": "79c7a5af-e185-4a65-e326-013139afd1da"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['i felt so sad'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "\n",
        "print(f\"\\nRun time: {end - start}\")"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i felt so sades like\n",
            "tiches el is sraythanesting itín thion your nowe you hlveny wintít love allfune hearing\n",
            "if thee the kurt\n",
            "the warplops of on\n",
            "you whinghate\n",
            "whee havee\n",
            "lowely of heasize\n",
            "now you stiflinn spuns you a diks pealf.\n",
            "\n",
            "frinc reeselld noft ckoplongafters\n",
            "full your lageders\n",
            "whint it saly han\n",
            "ded an fach h ar fould you\n",
            "\n",
            "ime fore. iver ablits\n",
            "you hav skerinnawlige a loveand\n",
            "\n",
            "of lly you an bets end yours. you.\n",
            "so ken whthad the canin elousande it still an thimemunstill\n",
            "\n",
            "hesth wi stply to comary onavem on owhel\n",
            "i hevs waro in so\n",
            "ker macas bivey\n",
            "\n",
            "\n",
            "- sond wirging liver irs enound de rat in therr boingired clo ke potelf the stow tood arous whonged\n",
            "bow lite a wryoul\n",
            "cum of you wers the you atre\n",
            "toll whowe in thay no\n",
            "tee hell\n",
            "awe you\n",
            "sull. one set\n",
            "quesello gayte\n",
            "\n",
            "\n",
            "- chaed if anching. in the mbe ters it sitce\n",
            "sull kads deats. sind bowthinim is it you pole\n",
            "is\n",
            "i chould\n",
            "1x3 dowerfuregilg\n",
            "but de\n",
            "never wirl the roillkes the lilesty im hund thenísts\n",
            "like thand you apl o fthecakring\n",
            "by endith thithsith of  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 1.8513760566711426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMUYy8Y2Hu-e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtpFxd5PHu6k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}